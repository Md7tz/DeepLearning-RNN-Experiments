{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b80b95be-ed93-4745-a1da-accb76573965",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q nltk | grep -v \"already\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3110e03-66d1-45bd-93d0-9f87fdd47a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Medhat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Medhat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Medhat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Medhat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8b34a0f-0fbe-4672-904d-37da1abb5d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "tokens = sample_text.lower().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3570d31a-033d-4235-990a-0ac02b9e354b",
   "metadata": {},
   "source": [
    "### GloVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57f22344-ccc9-4645-b46a-89497b6b8a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.38497   0.80092   0.064106 -0.28355  -0.026759 -0.34532  -0.64253\n",
      " -0.11729  -0.33257   0.55243  -0.087813  0.9035    0.47102   0.56657\n",
      "  0.6985   -0.35229  -0.86542   0.90573   0.03576  -0.071705 -0.12327\n",
      "  0.54923   0.47005   0.35572   1.2611   -0.67581  -0.94983   0.68666\n",
      "  0.3871   -1.3492    0.63512   0.46416  -0.48814   0.83827  -0.9246\n",
      " -0.33722   0.53741  -1.0616   -0.081403 -0.67111   0.30923  -0.3923\n",
      " -0.55002  -0.68827   0.58049  -0.11626   0.013139 -0.57654   0.048833\n",
      "  0.67204 ] (50,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preprocess_text(text: str) -> list:\n",
    "    # Convert to lower case\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = ''.join(c for c in text if c not in '.,:;-')\n",
    "    tokens = word_tokenize(text)\n",
    "    # print(tokens)\n",
    "    # Remove Stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "def load_glove_model(file) -> dict:\n",
    "    # init an empty dict to store \"word\" as key and its \"embedding\" as value.\n",
    "    glove_model = {}\n",
    "    with open(file,'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            split_line = line.split()\n",
    "            word = split_line[0]\n",
    "            embedding = np.array(split_line[1:], dtype=np.float64)\n",
    "            glove_model[word] = embedding\n",
    "    return glove_model\n",
    "\n",
    "embedding_dict = load_glove_model(\"./data/glove.6B.50d.txt\")\n",
    "\n",
    "# Let's check embeddings of a word\n",
    "hello_embedding = embedding_dict['hello']\n",
    "# Let's print the embedding vector dimension\n",
    "print(hello_embedding, hello_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "27230d40-0cec-4e63-a424-1399acc3cdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's create the embedding matrix for sample_text\n",
    "sample_tokens = preprocess_text(sample_text)\n",
    "sample_embedding_matrix = []\n",
    "\n",
    "for sample_token in sample_tokens:\n",
    "    sample_embedding_matrix.append(embedding_dict[sample_token])\n",
    "\n",
    "# we should have as many embedding vectors (rows of embedding matrix) as there are sample tokens\n",
    "assert len(sample_tokens) == len(sample_embedding_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3130c6b-bd06-4bd8-8dd7-078cec0e5faa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
