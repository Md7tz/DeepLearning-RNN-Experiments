{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8006246a-1bf1-478d-8cf7-7c9cf483fcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q nltk | grep -v \"already\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5696b8bd-f778-4486-95d9-8aa70b191728",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Medhat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Medhat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Medhat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Medhat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd2df97-7d47-4337-b5df-ff9723139886",
   "metadata": {},
   "source": [
    "### Load data\r\n",
    "\r\n",
    "Next, we need to load the data that we want to preprocess. In this example, we will use the following sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2579c83-7fc9-4333-9456-dae1a0ff772c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The quick brown fox jumped over the lazy dog.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8500cd-723a-4166-9719-2bb3432e90e9",
   "metadata": {},
   "source": [
    "### Text Normalization\n",
    "\n",
    "Text normalization is the process of converting text into a standard format. This involves converting all characters to lowercase and removing any punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfc9228a-d116-496f-bc31-c08c7fd076d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the quick brown fox jumped over the lazy dog'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to lowercase\n",
    "text = text.lower()\n",
    "\n",
    "# Remove punctuation\n",
    "text = ''.join(c for c in text if c not in '.,:;-')\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e435cd25-5147-4f9b-b0b7-30a0e8ced203",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Tokenization is the process of splitting a sentence into individual words or tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64c1ab38-2e34-413c-a737-d11f1e5a3235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f993e406-8abe-4f82-868b-5aba6fe814b1",
   "metadata": {},
   "source": [
    "### Stopword Removal\n",
    "\n",
    "Stopwords are common words that do not carry much meaning and can be removed from the text. We will use NLTK's list of stopwords and remove them from the tokenized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fbc41e2-cf7f-462d-9ce3-748bba946180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13c62977-7ec4-40fb-964a-49b319f0db32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quick', 'brown', 'fox', 'jumped', 'lazy', 'dog']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f12ae7-0ed6-4e46-a3b9-9e9e23c74914",
   "metadata": {},
   "source": [
    "### Stemming\r\n",
    "\r\n",
    "Stemming is the process of reducing a word to its base or root form. We will use Porter stemmer from NLTK for stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a5bbeb9-1bbb-4525-b96c-08bff0c7945b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quick', 'brown', 'fox', 'jump', 'lazi', 'dog']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "stemmed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214daeb4-2337-49c7-a192-6ad923581596",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "Lemmatization is the process of converting a word to its base or dictionary form. We will use WordNet lemmatizer from NLTK for lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6fa055cf-8706-47fa-ba27-8db6f8c4a419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quick', 'brown', 'fox', 'jumped', 'lazy', 'dog']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "lemmatizer_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47c3f10c-a60c-452f-8474-c4d843767970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:  the quick brown fox jumped over the lazy dog\n",
      "Tokenized text:  ['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n",
      "Filtered tokens:  ['quick', 'brown', 'fox', 'jumped', 'lazy', 'dog']\n",
      "Stemmed tokens:  ['quick', 'brown', 'fox', 'jump', 'lazi', 'dog']\n",
      "Lemmatized tokens:  ['quick', 'brown', 'fox', 'jumped', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "print(\"Original text: \", text)\n",
    "print(\"Tokenized text: \", tokens)\n",
    "print(\"Filtered tokens: \", filtered_tokens)\n",
    "print(\"Stemmed tokens: \", stemmed_tokens)\n",
    "print(\"Lemmatized tokens: \", lemmatized_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
